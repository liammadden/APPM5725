
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>Liam Madden</title><meta name="generator" content="MATLAB 9.3"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2018-10-18"><meta name="DC.source" content="Homework8.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h1>Liam Madden</h1><!--introduction--><p>Homework 8 Problem 1</p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">Abstract</a></li><li><a href="#2">Clear</a></li><li><a href="#3">Load</a></li><li><a href="#4">Format</a></li><li><a href="#5">Indices</a></li><li><a href="#6">Objective function (negative log-likelihood)</a></li><li><a href="#7">Training</a></li><li><a href="#8">Nesterov</a></li><li><a href="#9">fminsearch</a></li><li><a href="#10">Plot</a></li></ul></div><h2 id="1">Abstract</h2><p>We compare the gradient descent method from last week to the Nesterov accelerated method and to the Nedler-Mead, gradient-free, method. The Nesterov accelerated method converges much faster than the regular gradient descent method when it is given the right Lipshitcz constant. Without the right Lipschitz constant, the method doesn't even converge, instead oscillating around. The Nedler-Mead method, on the other hand, doesn't converge either without taking far too long. While the Nedler-Mead method is sure to converge eventually, it is slow, and while the Nesterov accelerated method is fast, it won't converge without the Liptschitz constant. Thus, both methods have their drawbacks.</p><h2 id="2">Clear</h2><pre class="codeinput">clear <span class="string">all</span>
close <span class="string">all</span>
</pre><h2 id="3">Load</h2><pre class="codeinput">load(<span class="string">'spamData.mat'</span>)
</pre><h2 id="4">Format</h2><pre class="codeinput">Xtrain = log(Xtrain+0.1);
Xtest = log(Xtest+0.1);
ytrain = 2*ytrain-1;
ytest = 2*ytest-1;
</pre><h2 id="5">Indices</h2><pre class="codeinput">p = length(Xtrain(1,:));
ntrain = length(Xtrain(:,1));
ntest = length(Xtest(:,1));
</pre><h2 id="6">Objective function (negative log-likelihood)</h2><pre class="codeinput">sigma = @(a) 1./(1+exp(-a));
muTrain = @(w) sigma(ytrain.*(Xtrain*w));
lTrain = @(w) -sum(log(muTrain(w)));
gradlTrain = @(w) -Xtrain'*(ytrain.*(1-muTrain(w)));
w0 = ones(p,1);
[h,test1,test2] = gradCheck(lTrain,gradlTrain,w0);
</pre><h2 id="7">Training</h2><pre class="codeinput">tol = .001;
w = gradDesc(w0,lTrain,gradlTrain,tol,10000,0,1);
</pre><h2 id="8">Nesterov</h2><pre class="codeinput">wn = accelNest(w0,lTrain,gradlTrain,tol,10000,1.8703e5,0);
</pre><h2 id="9">fminsearch</h2><pre class="codeinput">options.MaxFunEvals = 1e8;
options.MaxIter = 1e8;
w_fms = fminsearch(lTrain,w0,options);
</pre><h2 id="10">Plot</h2><pre class="codeinput">iter = [1:length(w(1,:))]';
itern = [1:length(wn(1,:))]';
figure(1)
plot(iter,log(lTrain(w(:,iter))),<span class="string">'-k'</span>,itern,log(lTrain(wn(:,itern))),<span class="string">'-r'</span>,iter,log(lTrain(w_fms))*ones(length(iter)),<span class="string">'-c'</span>)
xlabel({<span class="string">'Number of iterations'</span>,<span class="string">'The Nesterov accelerated method converges much quicker'</span>,<span class="string">'than the regular gradient descent method. However,'</span>,<span class="string">'it needed the exact Lipschitz constant in order to do'</span>,<span class="string">'so and wouldnt converge without it. On the other hand'</span>,<span class="string">'the Nelder-Mead method did not converge to the minimum, even'</span>,<span class="string">'after increasing the number of iterations.'</span>})
ylabel(<span class="string">'Log of negative log-liklihood'</span>)
legend(<span class="string">'Gradient Descent'</span>,<span class="string">'Nesterov Accelerated'</span>,<span class="string">'Nelder-Mead'</span>)
title(<span class="string">'Objective convergence'</span>)
</pre><img vspace="5" hspace="5" src="Homework8_01.png" alt=""> <p class="footer"><br><a href="http://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2017b</a><br></p></div><!--
##### SOURCE BEGIN #####
%% Liam Madden
% Homework 8
% Problem 1

%% Abstract
% We compare the gradient descent method from last week to the Nesterov
% accelerated method and to the Nedler-Mead, gradient-free, method. The
% Nesterov accelerated method converges much faster than the regular
% gradient descent method when it is given the right Lipshitcz constant.
% Without the right Lipschitz constant, the method doesn't even converge,
% instead oscillating around. The Nedler-Mead method, on the other hand,
% doesn't converge either without taking far too long. While the
% Nedler-Mead method is sure to converge eventually, it is slow, and while
% the Nesterov accelerated method is fast, it won't converge without the
% Liptschitz constant. Thus, both methods have their drawbacks.

%% Clear
clear all
close all

%% Load
load('spamData.mat')

%% Format
Xtrain = log(Xtrain+0.1);
Xtest = log(Xtest+0.1);
ytrain = 2*ytrain-1;
ytest = 2*ytest-1;

%% Indices
p = length(Xtrain(1,:));
ntrain = length(Xtrain(:,1));
ntest = length(Xtest(:,1));

%% Objective function (negative log-likelihood)
sigma = @(a) 1./(1+exp(-a));
muTrain = @(w) sigma(ytrain.*(Xtrain*w));
lTrain = @(w) -sum(log(muTrain(w)));
gradlTrain = @(w) -Xtrain'*(ytrain.*(1-muTrain(w)));
w0 = ones(p,1);
[h,test1,test2] = gradCheck(lTrain,gradlTrain,w0);

%% Training
tol = .001;
w = gradDesc(w0,lTrain,gradlTrain,tol,10000,0,1);

%% Nesterov
wn = accelNest(w0,lTrain,gradlTrain,tol,10000,1.8703e5,0);

%% fminsearch
options.MaxFunEvals = 1e8;
options.MaxIter = 1e8;
w_fms = fminsearch(lTrain,w0,options);

%% Plot
iter = [1:length(w(1,:))]';
itern = [1:length(wn(1,:))]';
figure(1)
plot(iter,log(lTrain(w(:,iter))),'-k',itern,log(lTrain(wn(:,itern))),'-r',iter,log(lTrain(w_fms))*ones(length(iter)),'-c')
xlabel({'Number of iterations','The Nesterov accelerated method converges much quicker','than the regular gradient descent method. However,','it needed the exact Lipschitz constant in order to do','so and wouldnt converge without it. On the other hand','the Nelder-Mead method did not converge to the minimum, even','after increasing the number of iterations.'})
ylabel('Log of negative log-liklihood')
legend('Gradient Descent','Nesterov Accelerated','Nelder-Mead')
title('Objective convergence')
##### SOURCE END #####
--></body></html>